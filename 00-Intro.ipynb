{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e65c61",
   "metadata": {},
   "source": [
    "# Straight Lines & JAX\n",
    "\n",
    "First, lets make some _messy_ straight line data, and see if we can recover the original line definition.\n",
    "\n",
    "We'll use JAX. JAX is a toolkit from DeepMind & Google used for NN research. Unlike `pytorch` and others it isn't a framework, more like a library and an ecosystem. This means we can see \"inside\" it. At a very basic level, you could think of it as `numpy`, but differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax # Access to the library\n",
    "import jax.numpy as jnp # Easy access to numpy like functions in jax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3a1d8",
   "metadata": {},
   "source": [
    "Lets create some data that is a straight line, but some random jitter in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random number tracking in JAX\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, new_key = jax.random.split(rng)\n",
    "\n",
    "slope = 3\n",
    "intercept = 2\n",
    "\n",
    "# Straight line with jitter\n",
    "n_items = 100\n",
    "x = jax.random.normal(rng, (n_items,))\n",
    "jitter = jax.random.normal(new_key, (n_items,))\n",
    "y = slope * x + intercept + 0.5 * jitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a3301",
   "metadata": {},
   "source": [
    "We have a slope of **3** and an intercept of **2**.\n",
    "\n",
    "To get ourselves comfortable with this, lets look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"({f_x:0.2f}, {f_y:0.2f})\" for f_x, f_y in list(zip(x,y))[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eda9e4",
   "metadata": {},
   "source": [
    "Of course - when we have this many points, our brain is not built to understand a sequence of numbers. Our eyes, however, are excellent big-data sensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "# plt.plot(x, 3 * x + 2, color=\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b34fb",
   "metadata": {},
   "source": [
    "## \"Exact\" solution\n",
    "\n",
    "We can code up the derivation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "beta_1 = (n * jnp.sum(x * y) - jnp.sum(x) * jnp.sum(y)) / (\n",
    "    n * jnp.sum(x**2) - jnp.sum(x) ** 2\n",
    ")\n",
    "beta_0 = (jnp.sum(y) - beta_1 * jnp.sum(x)) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988ffea",
   "metadata": {},
   "source": [
    "And the values of the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadd5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"beta_0: {beta_0:.2f}\")\n",
    "print(f\"beta_1: {beta_1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87672b",
   "metadata": {},
   "source": [
    "Again - our eyes are a lot better here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa1a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label=\"Data\", color=\"black\")\n",
    "plt.plot(x, 3 * x + 2, color=\"green\", label=\"Real Line\")\n",
    "plt.plot(x, beta_1 * x + beta_0, color=\"red\", label=\"Fitted Line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "# Add beta_0 and beta_1 values as text on the plot\n",
    "plt.text(0.05, 0.95, f\"beta_0 = {beta_0:.2f} (exact: {intercept})\\nbeta_1 = {beta_1:.2f} (exact: {slope})\", transform=plt.gca().transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c923937",
   "metadata": {},
   "source": [
    "## The Gradient\n",
    "\n",
    "Lets try gradient descent to solve this straight. First, lets look at how the gradient can be calculated in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2(beta):\n",
    "    x, y = beta\n",
    "    return x**2 + 2*y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x2(1, 1)= {x2((1,1))}\")\n",
    "print(f\"x2(2, 2)= {x2((2,2))}\")\n",
    "print(f\"x2(3, 3)= {x2((3,3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6732f",
   "metadata": {},
   "source": [
    "The `jax.grad` function will take the gradient of any function. That is all! A lot of magic happens:\n",
    "\n",
    "* A *tape* of the function is recorded.\n",
    "* JAX replays the tape and takes the derivative of every operation\n",
    "* Applies the product rule. A lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_of_x2 = jax.grad(x2)\n",
    "\n",
    "print(f\"grad_of_x2(1, 0)= {grad_of_x2((1.0,0.0))}\")\n",
    "print(f\"grad_of_x2(2, 0)= {grad_of_x2((2.0,0.0))}\")\n",
    "print(f\"grad_of_x2(3, 0)= {grad_of_x2((3.0,0.0))}\")\n",
    "print(f\"grad_of_x2(0, 1)= {grad_of_x2((0.0,1.0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78239d",
   "metadata": {},
   "source": [
    "* The expected values for the derivative of $x^2 \\rightarrow 2x$\n",
    "* Note that it takes the derivative for each of the arguments - so we get it w.r.t both $x$ and $y$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0e815",
   "metadata": {},
   "source": [
    "### Straight Line Fit\n",
    "\n",
    "Lets now use this to solve the straight line fit from earlier.\n",
    "\n",
    "First, lets define our $f(x)$. We'll call it `network` - because it will be a NN in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0423f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(beta, x):\n",
    "    b0, b1 = beta\n",
    "    return b0 + b1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b0d78",
   "metadata": {},
   "source": [
    "And our loss. We'll use $r$ from earlier - but we'll call it `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, x, y):\n",
    "    '''Calculate the mean squared error loss.\n",
    "\n",
    "    Args:\n",
    "        params (tuple): A tuple containing the model parameters (b0, b1).\n",
    "        x (array): The input features.\n",
    "        y (array): The true labels (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        float: The mean squared error loss.\n",
    "    '''\n",
    "    y_pred = network(params, x)\n",
    "    return jnp.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d4910",
   "metadata": {},
   "source": [
    "Here we define our *network* and *loss* function\n",
    "\n",
    "* Note the loss function is just the least squares function from before.\n",
    "* Note how nicely we can look at this and see what we are doing - nothing like the analytical function above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb2de9",
   "metadata": {},
   "source": [
    "Now that we know what JAX is going to do, this is a bit anti-climatic.\n",
    "\n",
    "But - this is a much more sophisticated function that the previous simple `x2`!\n",
    "* So we should still be impressed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c391fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_of_loss = jax.grad(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f025e",
   "metadata": {},
   "source": [
    "### Update the parameters\n",
    "\n",
    "Each iteration we calculate the gradient, and adjust the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(beta, x, y, i_epoch):\n",
    "    g = grad_of_loss(beta, x, y)\n",
    "    beta -= 0.1 * g\n",
    "    print(f\"Step {i_epoch}, loss {loss(beta, x, y)}\")\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d505ed87",
   "metadata": {},
   "source": [
    "Note the `0.1`:\n",
    "\n",
    "* This is the _learning rate_.\n",
    "* Adjust it to help converge more or less quickly\n",
    "* Too large can mean you miss the minimum\n",
    "* There are sophisticated algorithms that calculate different learning rates on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a5adc",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Lets loop 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426113de",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = jnp.array([1.0, 1.0])\n",
    "\n",
    "for i_epoch in range(10):\n",
    "    beta = one_epoch(beta, x, y, i_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final parameters: {beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0b466",
   "metadata": {},
   "source": [
    "Lets do a longer training and track the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = jnp.array([1.0, 1.0])\n",
    "beta_history = [beta]\n",
    "for i in range(50):\n",
    "    beta = one_epoch(beta, x, y, i)\n",
    "    beta_history.append(beta.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e653fb4",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_beta_0, nn_beta_1 = beta\n",
    "print(f\"beta_0 (b): {nn_beta_0:.2f} - least squares: {beta_0:.2f}\")\n",
    "print(f\"beta_1 (m): {nn_beta_1:.2f} - least squares: {beta_1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90895115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first coordinate from param_history\n",
    "first_coordinate = [b[0] for b in beta_history]\n",
    "\n",
    "# Plot the first coordinate as a function of the epoch number\n",
    "plt.plot(range(len(first_coordinate)), first_coordinate)\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(r\"$\\beta_0$ - Offset Coordinate\")\n",
    "plt.title(\"Offset Coordinate vs. Epoch Number\")\n",
    "plt.show()\n",
    "\n",
    "# Extract the first coordinate from param_history\n",
    "first_coordinate = [b[1] for b in beta_history]\n",
    "\n",
    "# Plot the first coordinate as a function of the epoch number\n",
    "plt.plot(range(len(first_coordinate)), first_coordinate)\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(r\"$\\beta_1$ - Slope Coordinate\")\n",
    "plt.title(\"Slope Coordinate vs. Epoch Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea53ddb",
   "metadata": {},
   "source": [
    "## With a NN\n",
    "\n",
    "Lets use a very simple fully connected NN and fit to the same data using JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flax optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import optax\n",
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8145d343",
   "metadata": {},
   "source": [
    "Lets define a module that will do our fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    features: list\n",
    "    '''Generate a fully connected multi-layer perceptron\n",
    "\n",
    "       Pass in `features` as a list of integers, where each integer\n",
    "       specifies the number of neurons in that layer.\n",
    "    '''\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.Dense(feat)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP(features=[16, 16, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b041d",
   "metadata": {},
   "source": [
    "Next initialize the network/function parameters with random numbers. Also, the input arrays need to be in a slightly funny form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca128d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Flax (needs shape [N,1])\n",
    "x_train = x.reshape(-1, 1)\n",
    "y_train = y.reshape(-1, 1)\n",
    "\n",
    "# Init the parameters\n",
    "mlp_key = jax.random.PRNGKey(0)\n",
    "params = mlp.init(mlp_key, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc912d3",
   "metadata": {},
   "source": [
    "Before we had a learning rate of 0.1. Now we will do an optimizer which modifies the learning rate as we go to help us get the best possible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd118ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.01)\n",
    "\n",
    "# Track the state (for optimizers that want to know things like variable momentum).\n",
    "state = train_state.TrainState.create(apply_fn=mlp.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc42279",
   "metadata": {},
   "source": [
    "Finally, the loss function! Which looks just like it did before.\n",
    "\n",
    "* JIT - \"Just In Time\" compiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    preds = mlp.apply(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f9d93",
   "metadata": {},
   "source": [
    "And the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af96472",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, x, y):\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params, x, y)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82361b3",
   "metadata": {},
   "source": [
    "Which we can now run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7242ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    state, loss = train_step(state, x_train, y_train)\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"[Flax] Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3c37c",
   "metadata": {},
   "source": [
    "And the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4180e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit\n",
    "x_plot = jnp.linspace(jnp.min(x), jnp.max(x), 100).reshape(-1, 1)\n",
    "y_pred = mlp.apply(state.params, x_plot).flatten()\n",
    "plt.scatter(x, y, label=\"Data\", color=\"black\", alpha=0.5)\n",
    "plt.plot(x_plot.flatten(), y_pred, color=\"purple\", label=\"Flax NN Fit\")\n",
    "plt.plot(x_plot.flatten(), 3 * x_plot.flatten() + 2, color=\"green\", label=\"Real Line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Flax NN Fit vs. True Line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97073909",
   "metadata": {},
   "source": [
    "NOTE: We lost the interpretation of what was going on - we don't have a slop!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit\n",
    "x_plot = jnp.linspace(jnp.min(x), jnp.max(x), 100).reshape(-1, 1)\n",
    "y_pred = mlp.apply(state.params, x_plot).flatten()\n",
    "plt.plot(x_plot.flatten(), y_pred, color=\"purple\", label=\"Flax NN Fit\")\n",
    "plt.plot(x_plot.flatten(), 3 * x_plot.flatten() + 2, color=\"green\", label=\"Real Line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Flax NN Fit vs. True Line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16f2b3",
   "metadata": {},
   "source": [
    "And it isn't flat! Where is the PHYSICS!!?!??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2bb79d",
   "metadata": {},
   "source": [
    "## Over Fitting\n",
    "\n",
    "Lets create a second dataset that is _independent_ of the first, and see how the loss of that varies over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_key2, _ = jax.random.split(new_key)\n",
    "new_key3, _ = jax.random.split(new_key2)\n",
    "\n",
    "x2 = jax.random.normal(new_key3, (1000,))\n",
    "jitter2 = jax.random.normal(new_key2, (1000,))\n",
    "y2 = slope * x2 + intercept + 0.5 * jitter2\n",
    "\n",
    "x_test = x2.reshape(-1, 1)\n",
    "y_test = y2.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8870e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the state to be random\n",
    "params2 = mlp.init(mlp_key, x_train)\n",
    "state2 = train_state.TrainState.create(apply_fn=mlp.apply, params=params2, tx=optimizer)\n",
    "\n",
    "# Run the training, tracking the loss for plotting.\n",
    "num_epochs = 500\n",
    "training_loss_by_epoch = []\n",
    "testing_loss_by_epoch = []\n",
    "for epoch in range(num_epochs):\n",
    "    state2, loss = train_step(state2, x_train, y_train)\n",
    "    training_loss_by_epoch.append(loss)\n",
    "\n",
    "    y_test_pred = mlp.apply(state2.params, x_plot)\n",
    "    test_loss = loss_fn(state2.params, x_test, y_test)\n",
    "    testing_loss_by_epoch.append(test_loss)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"[Flax] Epoch {epoch}, Loss: {loss:.4f}, test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0644d",
   "metadata": {},
   "source": [
    "Lets plot the test and training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15770788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing loss over epochs\n",
    "plt.plot(range(num_epochs), training_loss_by_epoch, label=\"Training Loss\")\n",
    "plt.plot(range(num_epochs), testing_loss_by_epoch, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale('log')\n",
    "plt.title(\"Training and Testing Loss vs. Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new fit...\n",
    "x_plot = jnp.linspace(jnp.min(x), jnp.max(x), 100).reshape(-1, 1)\n",
    "y_pred_train = mlp.apply(state.params, x_plot).flatten()\n",
    "y_pred_extra_fit = mlp.apply(state2.params, x_plot).flatten()\n",
    "plt.plot(x_plot.flatten(), y_pred_train, color=\"purple\", label=\"Flax NN Fit - Short Training\")\n",
    "plt.plot(x_plot.flatten(), y_pred_extra_fit, color=\"blue\", label=\"Flax NN Fit - Long Training\")\n",
    "plt.plot(x_plot.flatten(), 3 * x_plot.flatten() + 2, color=\"green\", label=\"Real Line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Flax NN Fit vs. True Line\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
